---
title: GitHub资源使用指南
date: 2021-08-25 15:58:27
permalink: /pages/6f82ef/
categories:
  - code
  - 版本控制与管理
tags:
  - 
---
# GitHub资源使用指南

[toc]

## 高效搜索开源项目

- 在搜索框里输入要检索的内容，然后在列表里翻页找自己需要的内容
- 进行筛选，在左侧加个语言的过滤项，更改搜索排列顺序
- 「高级搜索」需要在另外的界面里展开，进行二次搜索之类的。 

### **1. 明确搜索仓库名称、仓库描述、README**

GitHub 提供了便捷的搜索方式，可以限定只搜索仓库的标题、或者描述、README等。

我们可以直接限定关键字只查特定的地方。

如果想查找仓库名称含关键字的仓库，可以使用语法：`in:name` +关键词

如果想查找仓库描述含关键词的内容，可以使用语法：`in:descripton` +关键词

如果要查该文件包含特定关键词的话，可以使用语法：`in:readme `+关键词，也就是搜索README文件。

### **2. 明确搜索 star、fork 数大于多少的**

项目 star 数的多少，代表该项目有受欢迎程度。stars: **>** =数字 关键字。

如果想找 star 数大于 3000 的Spring Cloud 仓库，可以使用语法：`stars:>=3000 spring cloud`

如果不加 >= 的话，是要精确找 star 数等于具体数字的，这个一般有点困难。

如果想找在指定数字区间的话，可以使用语法：`stars: 10..20+关键词`

fork 数同理，将上面的 stars 换成 fork，其它语法相同

### **3. 明确搜索仓库大小的**

比如你只想看个简单的 Demo，不想找特别复杂的且占用磁盘空间较多的，可以在搜索的时候直接限定仓库的 size 。使用语法：`size:>=5000 +关键词`

其中，数字代表K， 5000代表着5M。

### **4. 明确仓库是否还在更新维护**

我们在确认是否要使用一些开源产品，框架的时候，是否继续维护是很重要的一点。

如果你想知道仓库是否还在更新维护，不在每个项目里看看最近 push 的时间，而是直接在搜索框指定**更新时间**在哪个时间前或后的，可以使用语法：` pushed:>2019-01-03 spring cloud`

你是想找指定时间之前或之后创建的仓库也是可以的，把 pushed 改成 **created** 就行。

### **5. 明确搜索仓库的 LICENSE**

开源不等于一切免费，不同的许可证要求也大不相同。 2018年就出现了 Facebook 修改 React 的许可协议导致各个公司纷纷修改自己的代码，寻找替换的框架。

如果我们要找协议最为宽松的 Apache License 2 的代码，可以使用语法：`license:apache-2.0 spring cloud`

其它协议就把apache-2.0替换一下即可，比如换成 mit 之类的。

### **6. 明确搜索仓库的语言**

如果想找找 Java 的库， 除了可以在左侧点击选择之外，还可以在搜索中过滤。可以使用语法： `language:java `

### **7.明确搜索某个人或组织的仓库**

如果想查看某杜嘉禾是否提交了新的功能，就可以指定其名称后搜索，可以使用语法：`user:Thaumy`

组合使用一下，把 Java 项目过滤出来，多个查询之间「空格」分隔即可。例如：

```
user:joshlong language:java
```

如果使想找某个组织的话，可以使用语法：`**org**:spring-cloud`

## 免费开源工具

### **音频编辑工具：Audacity**

如果我们需要制作一份自己的录音文件。系统录音机的编辑功能太薄弱，而用 Audition 又有些太过臃肿这是我们就可以试试Audacity，它是一款轻量级的录音编辑工具。

Audacity虽然是一个轻量级的工具，但声音效果、多轨编辑等功能一应俱全，并且支持安装插件来扩展功能。无论是简单的剪辑，还是高要求的制作，Audacity 都能胜任。

### **图片处理工具：Gimp**

如果我们需要制作文章配图，或是为自己的摄影作品调色，Gimp 就是一款功能强大且免费开源的图片处理工具。

Gimp 的功能十分强大，无论是制作图片还是对照片进行后期处理，Gimp 都可以轻松做到。即使别人发来了 PSD 文件，我们也可以用 Gimp 来打开。

### **Windows 优质应用合辑**

得益于系统的开放性，Windows 平台拥有极为庞大的应用数量。可 Windows 并没有一个好用的桌面应用商城。

Awesome Windows 涵盖了常用的 20 种应用分类，其中有不少都是免费，甚至开源的应用。Awesome Windows 为我们整理了大量优秀的 Windows 应用，我们可以依照分类找到自己需要的应用。

### **macOS 优质应用合辑**

尽管 macOS 拥有相对完善的 Mac App Store，但许多厂商仍选择独立发售应用。这使得 macOS 上的优质应用无法在 App Store 里得到一站式解决。

与 Awesome Windows 一样，Awesome macOS 也是一个优质应用的合集。在 macOS 应用大量改用订阅制的现在，我们不妨在这里找一些免费开源的替代品。如果你偏爱开源应用，也可以在 [这里](https://link.zhihu.com/?target=https%3A//github.com/serhii-londar/open-source-mac-os-apps) 找到优质的 macOS 开源应用。

### **Chrome 插件合辑**

ChromeAppHeroes 这个项目收录了不少国内外优秀的 Chrome 插件，并且还在持续更新中。如果你希望发现更多有趣、实用的 Chrome 浏览器插件

### **中国独立开发者作品合辑**

近年来，中国独立开发者的优质作品经常出现在大家眼中。无论是 JSBox 这样强大的应用，还是小黄条这样轻量级的应用，我们都可以在「中国独立开发者项目列表」中找到。

列表中包含了国内独立开发者的作品及其 GitHub 或博客，每个作品也有相应的开发状态和链接。我们可以通过这个列表与开发者取得联系，或是发现国内独立开发者的优秀作品。

### **设计工具合集**

Awesome Design Tools 这个项目为我们整理了海量的设计工具。从图形设计到动画设计，甚至是 AR 的制作，都可以在其中找到相应的工具。

这个列表囊括了 35 个类别的设计工具，其中既有 After Effects 这样的重量级应用，也不乏许多开源应用。

### **GitHub 秘籍**

如果你是 GitHub 新人，又想快速融入这个社区，不妨收下这份 GitHub 秘籍。让它带着你快速了解 GitHub 的使用技巧。

这个项目涵盖了 GitHub 上的许多实用小技巧，无论是如何克隆仓库，还是快速添加许可证，都可以在这份秘籍中找到。此外，GitHub 秘籍还包含 git 的一些使用技巧，让我们在管理自己的项目时可以事半功倍。

## 爬虫工具箱

> 开源最前线 、数据管道综合整理

最近国内一位开发者在 GitHub 上开源了个集众多数据源于一身的爬虫工具箱——InfoSpider

InfoSpider 是一个集众多数据源于一身的爬虫工具箱，旨在安全快捷的帮助用户拿回自己的数据，工具代码开源，流程透明。并提供数据分析功能，基于用户数据生成图表文件，使得用户更直观、深入了解自己的信息。

目前支持数据源包括：GitHub、QQ邮箱、网易邮箱、阿里邮箱、新浪邮箱、Hotmail邮箱、Outlook邮箱、京东、淘宝、支付宝、中国移动、中国联通、中国电信、知乎、哔哩哔哩、网易云音乐、QQ好友、QQ群、生成朋友圈相册、浏览器浏览历史、12306、博客园、CSDN博客、开源中国博客、简书。

在这样一个信息爆炸的时代，每个人都有很多个账号，账号一多就会出现这么一个情况：**个人数据分散在各种各样的公司之间，就会形成数据孤岛，多维数据无法融合，这个项目可以帮你将多维数据进行融合并对个人数据进行分析，这样你就可以更直观、深入了解自己的信息。**

作者已经开源了所有的项目代码及使用文档，并且在B站上还有使用视频讲解。

项目代码：https://github.com/kangvcar/InfoSpider

项目使用文档：https://infospider.vercel.app

项目视频演示：https://www.bilibili.com/video/BV14f4y1R7oF/

根据创建者介绍，InfoSpider 具有以下特性：

- **安全可靠：**本项目为开源项目，代码简洁，所有源码可见，本地运行，安全可靠。
- **使用简单：**提供 GUI 界面，只需点击所需获取的数据源并根据提示操作即可。
- **结构清晰：**本项目的所有数据源相互独立，可移植性高，所有爬虫脚本在项目的 Spiders 文件下。
- **数据源丰富：**本项目目前支持多达24+个数据源，持续更新。
- **数据格式统一：**爬取的所有数据都将存储为json格式，方便后期数据分析。
- **个人数据丰富：**本项目将尽可能多地为你爬取个人数据，后期数据处理可根据需要删减。
- **数据分析：**本项目提供个人数据的可视化分析，目前仅部分支持。

InfoSpider使用起来也非常简单，你只需要安装python3和Chrome浏览器，运行 python3 main.py，在打开的窗口点击数据源按钮, 根据提示选择数据保存路径，接着输入账号密码，就会自动爬取数据，根据下载的目录就可以查看爬下来的数据。

当然如果你想自己去练习和学习爬虫，作者也开源了所有的爬取代码，非常适合实战。

举个例子，比如爬取taobao的：

```text
import json
import random
import time
import sys
import os
import requests
import numpy as np
import math
from lxml import etree
from pyquery import PyQuery as pq
from selenium import webdriver
from selenium.webdriver import ChromeOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver import ChromeOptions, ActionChains
from tkinter.filedialog import askdirectory
from tqdm import trange


def ease_out_quad(x):
    return 1 - (1 - x) * (1 - x)

def ease_out_quart(x):
    return 1 - pow(1 - x, 4)

def ease_out_expo(x):
    if x == 1:
        return 1
    else:
        return 1 - pow(2, -10 * x)

def get_tracks(distance, seconds, ease_func):
    tracks = [0]
    offsets = [0]
    for t in np.arange(0.0, seconds, 0.1):
        ease = globals()[ease_func]
        offset = round(ease(t / seconds) * distance)
        tracks.append(offset - offsets[-1])
        offsets.append(offset)
    return offsets, tracks

def drag_and_drop(browser, offset=26.5):
    knob = browser.find_element_by_id('nc_1_n1z')
    offsets, tracks = get_tracks(offset, 12, 'ease_out_expo')
    ActionChains(browser).click_and_hold(knob).perform()
    for x in tracks:
        ActionChains(browser).move_by_offset(x, 0).perform()
    ActionChains(browser).pause(0.5).release().perform()

def gen_session(cookie):
    session = requests.session()
    cookie_dict = {}
    list = cookie.split(';')
    for i in list:
        try:
            cookie_dict[i.split('=')[0]] = i.split('=')[1]
        except IndexError:
            cookie_dict[''] = i
    requests.utils.add_dict_to_cookiejar(session.cookies, cookie_dict)
    return session

class TaobaoSpider(object):
    def __init__(self, cookies_list):
        self.path = askdirectory(title='选择信息保存文件夹')
        if str(self.path) == "":
            sys.exit(1)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36',
        }
        option = ChromeOptions()
        option.add_experimental_option('excludeSwitches', ['enable-automation'])
        option.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2})  # 不加载图片,加快访问速度
        option.add_argument('--headless')
        self.driver = webdriver.Chrome(options=option)
        self.driver.get('https://i.taobao.com/my_taobao.htm')
        for i in cookies_list:
            self.driver.add_cookie(cookie_dict=i)
        self.driver.get('https://i.taobao.com/my_taobao.htm')
        self.wait = WebDriverWait(self.driver, 20)  # 超时时长为10s

    # 模拟向下滑动浏览
    def swipe_down(self, second):
        for i in range(int(second / 0.1)):
            # 根据i的值，模拟上下滑动
            if (i % 2 == 0):
                js = "var q=document.documentElement.scrollTop=" + str(300 + 400 * i)
            else:
                js = "var q=document.documentElement.scrollTop=" + str(200 * i)
            self.driver.execute_script(js)
            time.sleep(0.1)

        js = "var q=document.documentElement.scrollTop=100000"
        self.driver.execute_script(js)
        time.sleep(0.1)

    # 爬取淘宝 我已买到的宝贝商品数据, pn 定义爬取多少页数据
    def crawl_good_buy_data(self, pn=3):

        # 对我已买到的宝贝商品数据进行爬虫
        self.driver.get("https://buyertrade.taobao.com/trade/itemlist/list_bought_items.htm")

        # 遍历所有页数

        for page in trange(1, pn):
            data_list = []

            # 等待该页面全部已买到的宝贝商品数据加载完毕
            good_total = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '#tp-bought-root > div.js-order-container')))

            # 获取本页面源代码
            html = self.driver.page_source

            # pq模块解析网页源代码
            doc = pq(html)

            # # 存储该页已经买到的宝贝数据
            good_items = doc('#tp-bought-root .js-order-container').items()

            # 遍历该页的所有宝贝
            for item in good_items:
                # 商品购买时间、订单号
                good_time_and_id = item.find('.bought-wrapper-mod__head-info-cell___29cDO').text().replace('\n', "").replace('\r', "")
                # 商家名称
                # good_merchant = item.find('.seller-mod__container___1w0Cx').text().replace('\n', "").replace('\r', "")
                good_merchant = item.find('.bought-wrapper-mod__seller-container___3dAK3').text().replace('\n', "").replace('\r', "")
                # 商品名称
                # good_name = item.find('.sol-mod__no-br___1PwLO').text().replace('\n', "").replace('\r', "")
                good_name = item.find('.sol-mod__no-br___3Ev-2').text().replace('\n', "").replace('\r', "")
                # 商品价格  
                good_price = item.find('.price-mod__price___cYafX').text().replace('\n', "").replace('\r', "")
                # 只列出商品购买时间、订单号、商家名称、商品名称
                # 其余的请自己实践获取
                data_list.append(good_time_and_id)
                data_list.append(good_merchant)
                data_list.append(good_name)
                data_list.append(good_price)
                #print(good_time_and_id, good_merchant, good_name)
                #file_path = os.path.join(os.path.dirname(__file__) + '/user_orders.json')
                # file_path = "../Spiders/taobao/user_orders.json"
                json_str = json.dumps(data_list)
                with open(self.path + os.sep + 'user_orders.json', 'a') as f:
                    f.write(json_str)

            # print('\n\n')

            # 大部分人被检测为机器人就是因为进一步模拟人工操作
            # 模拟人工向下浏览商品，即进行模拟下滑操作，防止被识别出是机器人
            # 随机滑动延时时间
            swipe_time = random.randint(1, 3)
            self.swipe_down(swipe_time)

            # 等待下一页按钮 出现
            good_total = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.pagination-next')))
            good_total.click()
            time.sleep(2)
            # while 1:
            #     time.sleep(0.2)
            #     try:
            #         good_total = self.driver.find_element_by_xpath('//li[@title="下一页"]')
            #         break
            #     except:
            #         continue
            # # 点击下一页按钮
            # while 1:
            #     time.sleep(2)
            #     try:
            #         good_total.click()
            #         break
            #     except Exception:
            #         pass

    # 收藏宝贝 传入爬几页 默认三页  https://shoucang.taobao.com/nodejs/item_collect_chunk.htm?ifAllTag=0&tab=0&tagId=&categoryCount=0&type=0&tagName=&categoryName=&needNav=false&startRow=60
    def get_choucang_item(self, page=3):
        url = 'https://shoucang.taobao.com/nodejs/item_collect_chunk.htm?ifAllTag=0&tab=0&tagId=&categoryCount=0&type=0&tagName=&categoryName=&needNav=false&startRow={}'
        pn = 0
        json_list = []
        for i in trange(page):
            self.driver.get(url.format(pn))
            pn += 30
            html_str = self.driver.page_source
            if html_str == '':
                break
            if '登录' in html_str:
                raise Exception('登录')
            obj_list = etree.HTML(html_str).xpath('//li')
            for obj in obj_list:
                item = {}
                item['title'] = ''.join([i.strip() for i in obj.xpath('./div[@class="img-item-title"]//text()')])
                item['url'] = ''.join([i.strip() for i in obj.xpath('./div[@class="img-item-title"]/a/@href')])
                item['price'] = ''.join([i.strip() for i in obj.xpath('./div[@class="price-container"]//text()')])
                if item['price'] == '':
                    item['price'] = '失效'
                json_list.append(item)
        # file_path = os.path.join(os.path.dirname(__file__) + '/shoucang_item.json')
        json_str = json.dumps(json_list)
        with open(self.path + os.sep + 'shoucang_item.json', 'w') as f:
            f.write(json_str)

    # 浏览足迹 传入爬几页 默认三页  https://shoucang.taobao.com/nodejs/item_collect_chunk.htm?ifAllTag=0&tab=0&tagId=&categoryCount=0&type=0&tagName=&categoryName=&needNav=false&startRow=60
    def get_footmark_item(self, page=3):
        url = 'https://www.taobao.com/markets/footmark/tbfoot'
        self.driver.get(url)
        pn = 0
        item_num = 0
        json_list = []
        for i in trange(page):
            html_str = self.driver.page_source
            obj_list = etree.HTML(html_str).xpath('//div[@class="item-list J_redsList"]/div')[item_num:]
            for obj in obj_list:
                item_num += 1
                item = {}
                item['date'] = ''.join([i.strip() for i in obj.xpath('./@data-date')])
                item['url'] = ''.join([i.strip() for i in obj.xpath('./a/@href')])
                item['name'] = ''.join([i.strip() for i in obj.xpath('.//div[@class="title"]//text()')])
                item['price'] = ''.join([i.strip() for i in obj.xpath('.//div[@class="price-box"]//text()')])
                json_list.append(item)
            self.driver.execute_script('window.scrollTo(0,1000000)')
        # file_path = os.path.join(os.path.dirname(__file__) + '/footmark_item.json')
        json_str = json.dumps(json_list)
        with open(self.path + os.sep + 'footmark_item.json', 'w') as f:
            f.write(json_str)

    # 地址
    def get_addr(self):
        url = 'https://member1.taobao.com/member/fresh/deliver_address.htm'
        self.driver.get(url)
        html_str = self.driver.page_source
        obj_list = etree.HTML(html_str).xpath('//tbody[@class="next-table-body"]/tr')
        data_list = []
        for obj in obj_list:
            item = {}
            item['name'] = obj.xpath('.//td[1]//text()')
            item['area'] = obj.xpath('.//td[2]//text()')
            item['detail_area'] = obj.xpath('.//td[3]//text()')
            item['youbian'] = obj.xpath('.//td[4]//text()')
            item['mobile'] = obj.xpath('.//td[5]//text()')
            data_list.append(item)
        # file_path = os.path.join(os.path.dirname(__file__) + '/addr.json')
        json_str = json.dumps(data_list)
        with open(self.path + os.sep + 'address.json', 'w') as f:
            f.write(json_str)


if __name__ == '__main__':
    # pass
    cookie_list = json.loads(open('taobao_cookies.json', 'r').read())
    t = TaobaoSpider(cookie_list)
    t.get_orders()
    # t.crawl_good_buy_data()
    # t.get_addr()
    # t.get_choucang_item()
    # t.get_footmark_item()
```

